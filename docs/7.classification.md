## Classification

划分规则应当简单一些, 其 VC-dim 更低, $\epsilon$-net 越小; 而 VC-dim 越高, 越容易出现过拟合. 因此这里考虑线性模型.

### 模型

**模型**: $y(x)=w^Tx+b$, 不妨记为 $y(x)=[w^T, b]^T[x, 1]=\widetilde{w}^T\widetilde{x}$, 因此在 $d+1$ 维空间上, 就是一个经过原点的超平面.

在上述模型的基础上, 希望找到一个经过原点的超平面, 并且假设模型是线性可分的, 所有数据都限制在单位球内(可做归一化得到).

输入数据是 $\{(x_i, y_i)|x_i \in R^{d+1}, y_i=\pm 1\}$

目标是找到 margin 最大的线性分类器 $w^*$. 这里 margin 是 $\gamma=\min\{y_i\left< w, x_i \right>\}$

### 感知机算法

其主要思想就是依次使用各数据点, 不断以"偏差" $y_i x_i$ 纠正 $w$

##### 算法步骤

1. Initialize. $w=y_s x_s$, $(x_s, y_s)$ 为 $X$ 中任取的一个数据. 令 $i=1, t=0$
2. 重复下列步骤直到 $t=T$
    1. 若 $y_i \left< x_i, w \right> < 0$, 则 $w\leftarrow w+y_ix_i $, $t\leftarrow t+1$
    2. $i \leftarrow (i+1) \% n$
3. 返回 $w=\frac{w}{\|w\|}$

##### 正确性分析

1. $\|w\|^2\le t$:
    $w_t=w_{t-1}+y_ix_i$, 故
    $$\begin{aligned}
    \|w_t\|^2 &= \|w_{t-1}+y_ix_i\|^2 \\
    &=\|w_{t-1}\|^2+2\left< w_{t-1}, y_ix_i \right> + \|y_i x_i\|^2 \\
    &\le \|w_{t-1}\|^2 + 1
    \end{aligned}$$
2. $\left< w_t, w^* \right> \ge t\gamma^*$ ($\gamma^*$ 是 $w^*$ 对应的 Margin)
    $$\begin{aligned}
    \left< w_t, w^* \right> &= \left< w_{t-1}+y_ix_i, w^* \right> \\
    &= \left< w_{t-1}, w^* \right> + \left< y_ix_i, w^* \right> \\
    &\ge \left< w_{t-1}, w^* \right> + \gamma^* \\
    &\ge t\gamma^*
    \end{aligned}$$

因为 $\|w^* \|=1$ $t\gamma^* \le \left< w_t, w^* \right> \le \left< w_t, \frac{w_t}{\|w_t\|} \right>=\frac{\|w_t\|^2}{\|w_t\|}=\|w_t\| \le \sqrt{t}$, 
因此只需要 $t \le \frac{1}{(\gamma^*)^2}$

因此复杂度是 $\Theta(nd\frac{1}{(\gamma^*)^2}$

#### SVM(Support Vector Machine)

目标是找到 Margin 最大的分类器, 离平面 $H$ 最近的数据都被称为 **support vector**, 其个数 $\le d+1$(意为 $d+1$ 个就能确定 d 维超平面)






    
